{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Transformation\n",
    "The main idea behind the Query Transformation is that translate/transform the user query in a way that the LLM can correctly answer the question. For instance, if the user asks an ambiguous question, our RAG retriever might retrieve incorrect (or ambiguous) documents based on the embeddings that are not very relevant to answer the user question, leading the LLM to hallucinate answers. There are few ways to tackle this problem. Some of them are,\n",
    "\n",
    "Step-back prompting: This involves encouraging the LLM to take a step back from a given question or problem and pose a more abstract, higher-level question that encompasses the essence of the original inquiry.\n",
    "\n",
    "Least-to-most prompting: This allows to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems.\n",
    "\n",
    "Query re-writing (Multi-Query or RAG Fusion): This allows to generate multiple questions from the original question with different wording and perspectives. Then retrieve documents using the similarity scores between each question and the vector store to answer the orginal question.\n",
    "\n",
    "A blog post about query transformation by Langchain can be found [here](https://blog.langchain.dev/query-transformations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Importing libraries\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebanaryal/RAG/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#@ load the document\n",
    "loader = PyPDFLoader(\"../Introduction/introduction-to-natural-language-processing.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#@ split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "text_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "#@ create a vector store\n",
    "vector_store = Chroma.from_documents(documents=text_chunks, \n",
    "                                     embedding=embeddings,\n",
    "                                     persist_directory=\"data/vectorstore\"\n",
    "                                     )\n",
    "\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Retrieve\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Translation\n",
    "- Multi-Query\n",
    "In multi-query approach, we first use an LLM (here I'm using ```llama3```) to generate 5 different questions based on our original question. To do that, we create a prompt and encapsulate it with the ```PromptTemplate```. Then we create the chain using LCEL, to read the user input and assign it to the question placeholder of the prompt, send the prompt to the LLM, parse the output containing 5 questions seperated by new line charcters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I'm using local llama model instead of an OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "MODEL = \"llama3\"\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an intelligent assistant. Your task is to generate 5 questions based on the provided question in different wording and different perspectives to retrieve relevant\n",
    " documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity \n",
    " search. Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    {\n",
    "        \"question\" : RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether or not our query generation works by invoking the created chain with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are five alternative questions that capture different perspectives and wording:',\n",
       " '',\n",
       " 'What is the evolution of human-computer interaction through natural language processing?',\n",
       " '',\n",
       " 'How did NLP develop from its early beginnings to its current applications in text analysis and generation?',\n",
       " '',\n",
       " 'Can you provide an overview of the milestones and advancements in the field of computational linguistics, particularly as it relates to natural language processing?',\n",
       " '',\n",
       " 'What are some key events or breakthroughs that have shaped the course of natural language processing research over the years?',\n",
       " '',\n",
       " 'How do historical developments in areas like machine learning, artificial intelligence, and computer science intersect with the emergence of natural language processing as a distinct field?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries.invoke(\"What are the brief history of Natural language processing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the 5 questions, we parallelly retrieve the most relevant 5 documents for each question (resulting in a list of lists) and create a new document list by taking the unique documents of the union of all the retrieved documents. To do that we create another chain, retrieval_chain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import loads, dumps\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_union(docs: List[List]):\n",
    "    all_docs = [dumps(d) for doc in docs for d in doc]\n",
    "    unique_docs = list(set(all_docs))\n",
    "    \n",
    "    return [loads(doc).page_content for doc in unique_docs] # We only return page contents\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | get_context_union\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebanaryal/RAG/venv/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"CO3354 Introduction to natural language processing\\nCreate a variable soysents containing all sentences from reports concerning soy\\nproducts.\\nreuters.categories()\\nPick out categories relating to soy:\\nsoysents = reuters.sents(categories=['soy-meal', ...])\\nDisplay the ﬁrst ten sentences in soysents .\\nprint soysents[:10]\\nCreate a variable metalwords containing all words from reports concerning\\nmetals.\\nmetalwords = reuters.words(categories = ['alum','copper','gold', ...])\",\n",
       " 'CNJ (2324) PRO (2243) ‘,’ (1913) ‘.’ (1892) ADV (1485) NP (1224) VN (952)\\n. . .\\n42',\n",
       " 'Longmans and Chambers, and the British Library.\\nCOBUILD (Bank of English) The Bank of EnglishTMforms part of the Collins\\nCorpus, developed by Collins Dictionaries and the University of Birmingham,\\nand contains 650 million words.\\nGutenberg An archive of free electronic books in various formats hosted at\\nhttp://www.gutenberg.org /\\nPenn Treebank A corpus of reports from the Wall Street Journal and other sources\\nin various different formats.\\n3.4 Some uses of corpora',\n",
       " 'textbook; Chapters 2 and 3 are especially relevant for this topic.\\nMcEnery and Hardie (2011) Corpus Linguistics: Method, Theory and Practice . Chapter\\n3 addresses the important topic of research ethics for corpus linguistics, which is\\noften neglected in technical or academic presentations of the subject.\\n3.1 Learning outcomes\\nBy the end of this chapter, and having completed the Essential reading and activities,\\nyou should be able to:',\n",
       " '3. critically appraise existing Natural Language Processing (NLP) applications such\\nas chatbots and translation systems\\n4. describe some applications of statistical techniques to natural language analysis,\\nsuch as classiﬁcation and probabilistic parsing.\\nEach main chapter contains a list of learning outcomes speciﬁc to that chapter at the\\nbeginning, as well as a summary at the end of the chapter.\\n1.4 Reading list and other learning resources',\n",
       " 'andcount is the number of tokens of that type in a text. A conditional frequency\\ndistribution is a collection of frequency distributions, each one for a different\\ncondition.\\nFor this example we add a second document to the corpus, extracted from a PDF\\n‘Guide to data protection’.\\nStep 1 Create a single variable text word consisting of pairs of each word-token\\nwith the ﬁleid of the document it occurs in.\\n40',\n",
       " 'tagging: associating each word in a text with a grammatical category or part of\\nspeech.\\n1.6.4 Chapter 5: Statistically-based techniques for text analysis\\nStatistical and probabilistic methods are pervasive in modern computational\\nlinguistics. These methods generally do not aim at complete understanding or\\nanalysis of a text, but at producing reliable answers to well-deﬁned problems such as\\nsentiment analysis, topic detection or recognising named entities and relations\\nbetween them in a text.',\n",
       " 'probably also adopt different styles when talking face-to-face and on the telephone.\\nLiterary scholars as well as law enforcement and intelligence agencies may have an\\ninterest in identifying the author of a document from internal evidence. There have\\nbeen various famous and less well-known instances of controversial attribution of\\nauthorship: for example, various ﬁgures have been put forward as the authors of\\nShakespeare’s plays.\\n3.4.4 Training and evaluation',\n",
       " 'Corpora\\n3.5.4 Penn Treebank\\nThe Penn Treebank with its various offshoots is one of the widely used linguistic\\nresources among empirical researchers.\\nIt includes a collection of texts in four different formats:\\nRaw text (original).\\nTagged with POS using a tagset which was developed as part of the project.\\n‘Parsed’; that is, marked up with constituent structure.\\nCombined, including both POS tags and constituent structure.',\n",
       " '5http://www.itl.nist.gov/iaui/894.02/related projects/muc/; last visited 27th May 2013\\n27',\n",
       " 'generate an illusion that users are interacting with a human rather than a machine\\n(Hayes and Ford, 1995). Hopefully, by the time you ﬁnish this course you will have\\ncome to appreciate some of the challenges posed by full understanding of natural\\nlanguage as well as the very real achievements that have resulted from focusing on a\\nrange of speciﬁc, well-deﬁned tasks.\\n1.2 Aims of the course\\nThis course combines a critical introduction to key topics in theoretical linguistics',\n",
       " 'software modules written in Python and a collection of corpora and other\\nresources. See section 1.5 below for advice on installing the NLTK and other\\nsoftware packages.\\nIn the course of working through this text you will gain some experience and\\nfamiliarity with the Python language, though you will not be expected to\\nproduce substantial original code as part of the learning outcomes of the course.\\nRecommended reading',\n",
       " 'This is a list of textbooks and other resources which will be useful for all or most\\nparts of the course. Additional readings will be given at the start of each chapter. See\\nthe bibliography for a full list of books and articles referred to, including all ISBNs.\\nIn some cases several different books will be listed: you are not expected to read all\\nof them, rather the intention is to give you some alternatives in case particular texts\\nare hard to obtain.\\nEssential reading',\n",
       " 'sequence – to do with the order in which items occur: may include a wildcard\\ncharacter which is written as the period or full stop ‘.’ and may be replaced by\\nany character.\\nselection – specifying a choice between alternative items or sequences, indicated by\\nthe ‘j’ operator\\niteration – repetition of items or sequences, indicated by the ‘*’ operator, meaning\\nzero or more occurrences of whatever precedes the star.\\nSome simple examples:',\n",
       " 'A version supporting Python 3 is under development and may be available for\\ntesting by the time you read this guide (as of April 2013).\\n1.6 How to use the guide/structure of the course\\nThis section gives a brief summary of each chapter. These learning outcomes are\\nlisted at the beginning of each main chapter and assume that you have worked\\nthrough the recommended readings and activities for that chapter.\\n1.6.1 Chapter 2: Introducing NLP: patterns and structures in language',\n",
       " 'CO3354 Introduction to natural language processing\\n,)\\n(VP will\\n(VP join\\n(NP the board)\\n(PP-CLR as\\n(NP a nonexecutive director))\\n(NP-TMP Nov. 29)))\\n.))\\nCombined\\n( (S\\n(NP-SBJ\\n(NP (NNP Pierre) (NNP Vinken) )\\n(, ,)\\n(ADJP\\n(NP (CD 61) (NNS years) )\\n(JJ old) )\\n(, ,) )\\n(VP (MD will)\\n(VP (VB join)\\n(NP (DT the) (NN board) )\\n(PP-CLR (IN as)\\n(NP (DT a) (JJ nonexecutive) (NN director) ))\\n(NP-TMP (NNP Nov.) (CD 29) )))\\n(. .) ))\\n3.5.5 Gutenberg archive',\n",
       " 'Activity: Context-free grammar . . . . . . . . . . . . . . . . . . . . . . 23\\n2.4.5 Looking ahead: some further uses of regular expressions . . . . 23\\ni',\n",
       " '5. Some simple techniques for analysing corpora include concordancing,\\ncollocations and (conditional) frequency distributions. None of these techniques\\ninvolve automated linguistic analysis: the interpretation of the outputs is down\\nto the analyst.\\n3.8 Sample examination question\\na) Explain what is meant by the following types of corpus, and describe an example\\nin each category that you have encountered during this course:\\nisolated\\ncategorised\\noverlapping\\ntemporal.',\n",
       " 'text. We know that personal names always start with capital letters, but that is not\\nenough to distinguish them from names of countries, cities, companies, racehorses\\n12',\n",
       " 'In addition to the applications listed above, corpora are routinely used in linguistic\\nresearch for training and testing machine-learning systems for speciﬁc tasks in text\\nanalytics such as:\\ndetecting the topic of a document\\nanalysing the sentiments expressed for or against some product or policy\\nidentifying individuals described in a text, relations between them and events\\nthey participate in\\nstatistical parsing\\nstatistical machine translation.',\n",
       " 'describing patterns of various kinds that occur in texts. Subsequently in section 2.4\\nwe will begin to motivate the analysis of texts in terms of hierarchical structures in\\nwhich elements of various kinds can be embedded within each other, in a\\ncomparable way to the elements that make up an HTML web document. This section\\nintroduces some technical machinery such as: ﬁnite-state machines (FSMs), regular\\nexpressions, regular grammars and context-free grammars.\\n2.3 Basic concepts',\n",
       " 'The NLTK includes a small selection of out-of-copyright literary texts from Project\\nGutenberg, an archive of free electronic books hosted at http://www.gutenberg.org/\\nSome of the texts included are:\\nJane Austen: Emma ,Persuasion\\nGK Chesterton: Father Brown stories ,The Man Who Was Thursday\\nWilliam Blake: Poems\\nMilton: Paradise Lost\\nShakespeare: Julius Caesar ,Macbeth ,Hamlet\\n3.5.6 Other corpora\\nSome further corpora included with the NLTK are:',\n",
       " 'which we mean words, numbers, punctuation and other meaningful symbols\\nthat are printed on paper or some other ﬂat surface, or displayed on a screen.\\n2. Some fundamental operations in text analysis include tokenisation , which\\ninvolves extracting these meaningful elements from a stream of electronic\\ncharacters and discarding white space, line feed characters and other material\\nwhich has no explicit meaning for a human reader, and using regular\\nexpressions to identify patterns in a text.',\n",
       " '2. List the different categories within the corpus.\\n3. Count the number of sentences in the science ﬁction category.\\n4. Extract all the word tokens from the science ﬁction category, paired with their\\ntags, and store them in the variable bsf. Note that the simpliﬁed tagset is\\nselected.\\n5. Calculate a frequency distribution of the tags: this gives an ordered list of the\\ntags paired with their frequency in the variable sftagfd. (Only the 12 most\\nfrequent are shown.)',\n",
       " 'CO3354 Introduction to natural language processing\\nYour turn . You may also ﬁnd it useful to attempt some of the exercises provided at\\nthe end of each chapter.\\nFrom this chapter onwards you will be running Python sessions and using the NLTK.\\nYou should get into the habit of starting sessions with the following commands:\\n>>> from __future__ import division\\n>>> import nltk, re, pprint\\nOne of the features that makes the Python language suitable for natural language',\n",
       " 'been developed within computational linguistics and we will see some examples in\\nsubsequent chapters. The following is a list of categories that are often encountered\\nin general linguistics: you will be familiar with many of them already from learning\\nthe grammar of English or other languages, though some terms such as Determiner\\norConjunction may be new to you.\\nNoun ﬁsh, book, house, pen, procrastination, language\\nProper noun John, France, Barack, Goldsmiths, Python',\n",
       " 'Some uses of corpora\\nor ‘the entire population of texts from which we will take our samples’ (McEnery and\\nWilson, 2001, p. 78) and calculating the size of the corpus that is necessary for it to\\nbe maximally representative. The sampling frame may, for example, be bibliographic ,\\nbased on some comprehensive index or the holdings of a particular library, or\\ndemographic , selecting informants on the basis of various social categories as is often\\ndone in public opinion research.',\n",
       " 'applications is the very ﬂexible treatment of data structures such as lists, strings and\\nsequences. You should be familiar with these structures from previous programming\\ncourses, but should ensure you understand the way they are handled in Python. For\\nthis chapter, only lists are relevant and you should study Bird et al. (2009, section\\n1.2) before trying any of the learning activities in this chapter.\\n3.3 Corpora and other data resources',\n",
       " 'This chapter looks at different approaches to analysing texts, ranging from ‘shallow’\\ntechniques that focus on individual words and phrases to ‘deeper’ methods that\\nproduce a full representation of the grammatical structure of a sentence as a\\nhierarchical tree diagram. The chapter introduces two important formalisms:\\nregular expressions , which will play an important part throughout the course, and\\ncontext-free grammars which we return to in Chapter 6 of the subject guide.',\n",
       " 'Parsers are computer programs that use grammar rules to analyse sentences, and\\nthis chapter introduces some fundamental approaches to syntactic parsing.\\n1.6.6 Appendices\\nThe Appendices include:\\nA. A bibliography listing all works referenced in the subject guide, including\\npublication details and ISBNs.\\nB. A glossary of technical terms used in this subject guide.\\nC. Answers to selected activities.\\nD. A trace of a recursive descent parse as described in Chapter 6 of the subject guide.',\n",
       " 'languages. http://corpus.leeds.ac.uk/it/ (last visited 27th May 2013)\\nLearning activity\\n1. Pick two or three of the corpora mentioned above and research them online, focusing on questions\\nsuch as:\\nhow large is the corpus?\\nwhat language(s) and genre(s) does it represent?\\nwhen was it constructed and what is its intended use?\\nwhat is the sampling frame?\\nwhat level of interannotator agreement was achieved, if reported?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What are the brief history of Natural language processing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put all together by creating a one final chain to read the user query, get the contexts from 5 different documents using the retrieval_chain, add both the question and context to the prompt, send it through the LLM, and get the final formatted output using the StrOutputParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "    Asnwer the given question using the provided context and if you don't know the answer, just reply as I don't know.\n",
    "    Context: {context} \n",
    "    Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chain_query = (\n",
    "    {'context': retrieval_chain, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided context does not mention the brief history of Natural Language Processing (NLP). It seems to focus on text analysis, corpora, and related topics. If you're looking for information on the history of NLP, I can try to provide a general overview or suggest some resources where you might find this information.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_chain_query.invoke(\"What are the brief history of Natural language processing?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
