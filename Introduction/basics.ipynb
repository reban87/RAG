{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Importing libraries\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Since I'm using local llama model instead of an OpenAI\n",
    "from langchain.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vector Store using ChromaDB\n",
    "First we load all the PDF documents using ```PyPDFLoader``` through the ```PyPDFLoader()```. After loading we have to generate embeddings for each document to compare with the question when selecting the documents that provide relevant context. To do that, we first generate chunks using the ```RecursiveCharacterTextSplitter```, splitting the each document. Then we represent each chunk using ```OllamaEmbeddings``` embeddings that utilizes ```llama3``` model. Once the embedding vectors for each chunk generated it will be stored in a database (here we use local ChromaDB) called vecorstore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "loader = PyPDFLoader(\"introduction-to-natural-language-processing.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "text_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "#@ Vector store\n",
    "vectorstore = Chroma.from_documents(text_chunks, embedding=embeddings, persist_directory=\"db/vectorstore\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the vector store by calling its similarity_search method with a query as bellow. As you can see, we retrieved a list of for documents related to the question. Note that each document as several fields, namely, page_content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CO3354 Introduction to natural language processing\\nannotation, it is good practice to use multiple annotators for at least a sample of\\nthe corpus and report the level of inter-annotator agreement that was\\nachieved.\\n2. Some uses of corpora include:\\nLexicography (compiling dictionaries).\\nCompiling grammars for education and reference purposes.\\nStylistics: developing techniques to identify the author or genre of a\\ndocument; investigating the effect on language use of different channels', metadata={'page': 47, 'source': 'introduction-to-natural-language-processing.pdf'}),\n",
       " Document(page_content='CO3354 Introduction to natural language processing\\nYour turn . You may also ﬁnd it useful to attempt some of the exercises provided at\\nthe end of each chapter.\\nFrom this chapter onwards you will be running Python sessions and using the NLTK.\\nYou should get into the habit of starting sessions with the following commands:\\n>>> from __future__ import division\\n>>> import nltk, re, pprint\\nOne of the features that makes the Python language suitable for natural language', metadata={'page': 35, 'source': 'introduction-to-natural-language-processing.pdf'}),\n",
       " Document(page_content='generate an illusion that users are interacting with a human rather than a machine\\n(Hayes and Ford, 1995). Hopefully, by the time you ﬁnish this course you will have\\ncome to appreciate some of the challenges posed by full understanding of natural\\nlanguage as well as the very real achievements that have resulted from focusing on a\\nrange of speciﬁc, well-deﬁned tasks.\\n1.2 Aims of the course\\nThis course combines a critical introduction to key topics in theoretical linguistics', metadata={'page': 10, 'source': 'introduction-to-natural-language-processing.pdf'}),\n",
       " Document(page_content='been developed within computational linguistics and we will see some examples in\\nsubsequent chapters. The following is a list of categories that are often encountered\\nin general linguistics: you will be familiar with many of them already from learning\\nthe grammar of English or other languages, though some terms such as Determiner\\norConjunction may be new to you.\\nNoun ﬁsh, book, house, pen, procrastination, language\\nProper noun John, France, Barack, Goldsmiths, Python', metadata={'page': 18, 'source': 'introduction-to-natural-language-processing.pdf'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"What is natural language processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and generate relevant snippets from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from multiprocessing import context\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If the context is not relevant, just reply as \"I don't know\"\n",
    "\n",
    "context: {context}\n",
    "\n",
    "question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt | model | parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, I can answer that the Brown Corpus refers to a text corpus or collection of texts used for linguistic analysis and statistical modeling. It appears that this particular corpus contains two documents, one from an HTML file and another from a PDF file (\"Guide to data protection\").'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what is brown corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
